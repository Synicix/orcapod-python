{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orcapod as op\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14852fb6",
   "metadata": {},
   "source": [
    "We will also make heavy use of PyArrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0157ee4",
   "metadata": {},
   "source": [
    "### Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773c9d7",
   "metadata": {},
   "source": [
    "In this notebook, we will create a local directory called `pipeline_data` and store results in there. To make sure we get reproducibile results, we start by making sure that this directory does not exist locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"./pipeline_data\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f69204",
   "metadata": {},
   "source": [
    "### Creating streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86895f3",
   "metadata": {},
   "source": [
    "At the moment, there is only one way to create stream and that is by wrapping a PyArrow table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pydict(\n",
    "    {\n",
    "        \"a\": [1, 2, 3],\n",
    "        \"b\": [\"x\", \"y\", \"z\"],\n",
    "        \"c\": [True, False, True],\n",
    "        \"d\": [1.1, 2.2, 3.3],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac8f32",
   "metadata": {},
   "source": [
    "Use `op.streams.ImmutableTableStream` to turn table into a stream. You will also have to specify which columns are the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0394d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = op.streams.ImmutableTableStream(table, tag_columns=[\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac78cc",
   "metadata": {},
   "source": [
    "### Working with streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a854e7",
   "metadata": {},
   "source": [
    "Once you have a stream, you can iterate through tag, packet pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag, packet in stream:\n",
    "    print(f\"Tag: {tag}, Packet: {packet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7876b",
   "metadata": {},
   "source": [
    "You can also get all tag packet pairs as a list of tuples by calling `.flow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e67bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa500e",
   "metadata": {},
   "source": [
    "Every stream can be converted into a table with `as_table()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52baee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.as_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b29786",
   "metadata": {},
   "source": [
    "Optionally, you can pass in arguments to `as_table` to have system columns included in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b297f6",
   "metadata": {},
   "source": [
    "`include_source` adds `source` column for each data (non-tag) column patterned like `_source_{column}` and will contain information about where that particular value orginated from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.as_table(include_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec7b19",
   "metadata": {},
   "source": [
    "`include_content_hash` will compute `content_hash` for each packet and include it as `_content_hash` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.as_table(include_content_hash=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d74238",
   "metadata": {},
   "source": [
    "Alternatively, you can pass in a custom column name to use for the content hash column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b9e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.as_table(include_content_hash=\"my_hash_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7012c5a",
   "metadata": {},
   "source": [
    "Finally, `include_data_context` adds data context column as `_context_key` which captures information about the OrcaPod version, hasher version etc that were used when generting that packet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbfa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.as_table(include_data_context=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce05b68",
   "metadata": {},
   "source": [
    "### Tags and Packets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20783626",
   "metadata": {},
   "source": [
    "The tags and packets returned by the streams can be thought of as special dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78096a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags_and_packets = stream.flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag, packet = all_tags_and_packets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fa1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "packet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be117a",
   "metadata": {},
   "source": [
    "The element of tag/packet can be accessed just like dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42158816",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a792175",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "packet[\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "packet[\"d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992134a",
   "metadata": {},
   "source": [
    "They have a few methods that will provide additional insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56423d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns typespec (dictionary of key to type)\n",
    "packet.types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e02f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry names as strings\n",
    "packet.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70ee75",
   "metadata": {},
   "source": [
    "They can also be converted to an Arrow table by calling `as_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b18ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "packet.as_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4a38f",
   "metadata": {},
   "source": [
    "And schema is conveniently available as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "packet.arrow_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad91d0",
   "metadata": {},
   "source": [
    "You can also get a plain dictionary from tag/packet with `as_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fadd572",
   "metadata": {},
   "source": [
    "Packet contains some additional data such as `source_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f00feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "packet.source_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d755600",
   "metadata": {},
   "source": [
    "These additional data can be included when converting to dict or table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "packet.as_dict(include_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "packet.as_table(include_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab6fc7",
   "metadata": {},
   "source": [
    "The hash of tag/packet can be computed with `content_hash()` method. The result will be cached so that it won't be computed again unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03219fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag.content_hash()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ab6c6",
   "metadata": {},
   "source": [
    "## Working with operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9dd928",
   "metadata": {},
   "source": [
    "We start getting into orcapod computation when we use operators. At the time of the writing, only `Join` operator is implemented fully but more are to come very shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef99b67",
   "metadata": {},
   "source": [
    "Let's prepare two streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = pa.Table.from_pydict(\n",
    "    {\n",
    "        \"id\": [0, 1, 4],\n",
    "        \"a\": [1, 2, 3],\n",
    "        \"b\": [\"x\", \"y\", \"z\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "table2 = pa.Table.from_pydict(\n",
    "    {\n",
    "        \"id\": [0, 1, 2],\n",
    "        \"c\": [True, False, True],\n",
    "        \"d\": [1.1, 2.2, 3.3],\n",
    "    }\n",
    ")\n",
    "\n",
    "stream1 = op.streams.ImmutableTableStream(table1, tag_columns=[\"id\"])\n",
    "stream2 = op.streams.ImmutableTableStream(table2, tag_columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f87fcf3",
   "metadata": {},
   "source": [
    "We now join the two streams by instantiating the Join operator and then passing in the two streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "join = op.operators.Join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_stream = join(stream1, stream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24a492",
   "metadata": {},
   "source": [
    "Calling an operator on stream(s) immediately performs checks to make sure that the input streams are comaptible with the operator but otherwise it does NOT trigger any computation. Computation occurs only when you try to **access the output stream's content via iteration, flow, or through conversion to table**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092abff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag, packet in joined_stream:\n",
    "    print(f\"Tag: {tag}, Packet: {packet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095856e3",
   "metadata": {},
   "source": [
    "The output of the computation is automatically cached so that as long as you access the same output stream, you won't be triggering unnecessary recomputation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_stream.as_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869a1da",
   "metadata": {},
   "source": [
    "## Working with Function Pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7991a",
   "metadata": {},
   "source": [
    "Now we have explored the basics of streams, tags, packets, and operators (i.e. Join), it's time to explore the meat of `orcapod` -- `FunctionPod`s! Let's start by defining a very simple function pod that takes in two numbers and return the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35423d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op.function_pod(output_keys=[\"sum\"])\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"A simple function pod that adds two numbers.\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737eeac",
   "metadata": {},
   "source": [
    "You'll notice that, aside from the `op.function_pod` decorator, this is nothing but an ordinary Python function with type hints! The type hints are crucial however, as this will be used by `orcapod` system to validate the input streams into your pods and to be able to predict if the output of your pod can be fed into another operator/pod without an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf23360",
   "metadata": {},
   "source": [
    "Once you have function pod defined, you can already use it on streams just like operators. Let's prepare a stream that has entries for `a` and `b` and then feed them into the function pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = pa.Table.from_pydict(\n",
    "    {\n",
    "        \"id\": [0, 1, 2, 3, 4],\n",
    "        \"a\": [1, 2, 3, 4, 5],\n",
    "        \"b\": [10, 20, 30, 40, 50],\n",
    "    }\n",
    ")\n",
    "\n",
    "input_stream = op.streams.ImmutableTableStream(input_table, tag_columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the stream through the function pod!\n",
    "output_stream = add_numbers(input_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5beae2",
   "metadata": {},
   "source": [
    "And that's it! Believe it or not, that is all it takes to set up the computation. The actual computation will be triggered the first time you access the content of the output stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, p in output_stream:\n",
    "    print(f\"Tag: {t}, Packet: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff00efa",
   "metadata": {},
   "source": [
    "Simple, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0a24e",
   "metadata": {},
   "source": [
    "## Chaining operators and pods into a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa1e75",
   "metadata": {},
   "source": [
    "Now that we have seen how to define and run pods, it's time to put them together into a concrete pipeline. To do so, we will construct a `Pipeline` instance. When doing so, we have to pass in a place to save data to, so we will also prepare a data store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bc91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = op.stores.BatchedDeltaTableArrowStore(base_path=\"./pipeline_data\")\n",
    "\n",
    "pipeline = op.Pipeline(name=\"MyPipelin\", pipeline_store=data_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef281a1e",
   "metadata": {},
   "source": [
    "Once we have the pipeline ready, we can define the pipeline by simply running & chaining operators and pods **inside the pipeline context**. Typically, you'd want to define your function pods before hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@op.function_pod(output_keys=[\"sum\"])\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"A simple function pod that adds two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@op.function_pod(output_keys=[\"product\"])\n",
    "def multiply_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"A simple function pod that multiplies two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@op.function_pod(output_keys=[\"result\"])\n",
    "def combine_results(sum: int, product: int) -> str:\n",
    "    \"\"\"A simple function pod that combines results.\"\"\"\n",
    "    return f\"Sum: {sum}, Product: {product}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e132fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now defien the pipeline\n",
    "with pipeline:\n",
    "    sum_results = add_numbers(input_stream)\n",
    "    product_results = multiply_numbers(input_stream)\n",
    "    final_results = combine_results(sum_results, product_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad175c6",
   "metadata": {},
   "source": [
    "You can access individual elements of the pipeline as an attribute. By default, the attribute is named after the operator/pod name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33f5a9",
   "metadata": {},
   "source": [
    "Notice that elements of the pipeline is wrapped in a `Node`, being either `PodNode` or `KernelNode`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6bc8df",
   "metadata": {},
   "source": [
    "You can fetch results of the pipeline through these nodes. For example, you can access the saved results of the pipeline as Polars dataframe by access the `df` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21086f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_numbers.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920d65c",
   "metadata": {},
   "source": [
    "You'll notice that `df` comes back empty because the pipeline is yet to run. Let's now trigger the pipeline to fill the nodes with computation results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e741659",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4341d5",
   "metadata": {},
   "source": [
    "This will cause all nodes in the pipeline to run and store the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50891c40",
   "metadata": {},
   "source": [
    "Now let's take a look at the computed results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77154ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_numbers.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43931402",
   "metadata": {},
   "source": [
    "You now have the computations saved at each node!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82312bda",
   "metadata": {},
   "source": [
    "### Labeling nodes in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a8f8f",
   "metadata": {},
   "source": [
    "When constructing the pipeline, each invocation of the operator/pod results in a new node getting added, with the name of the node defaulting to the name of the operator/pod. If you use the same pod multiple times, then the nodes will be given names of form `{pod_name}_0`, `{pod_name}_1`, and so on.\n",
    "\n",
    "While this is helpful default behavior, you'd likely want to explicitly name each node so you can more easily understand what you are accessing within the pipeline. To achieve this, you can explicitly label each invocation with `label=` argument in the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e65e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = op.stores.BatchedDeltaTableArrowStore(base_path=\"./pipeline_data\")\n",
    "\n",
    "pipeline2 = op.Pipeline(name=\"MyPipelin\", pipeline_store=data_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now defien the pipeline\n",
    "with pipeline2:\n",
    "    sum_results = add_numbers(input_stream, label=\"my_summation\")\n",
    "    product_results = multiply_numbers(input_stream, label=\"my_product\")\n",
    "    final_results = combine_results(\n",
    "        sum_results, product_results, label=\"my_final_result\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f146ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2.my_summation.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2.my_product.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a918db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2.my_final_result.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380dad8",
   "metadata": {},
   "source": [
    "Notice that despite just freshly creating the pipeline, each node already had results filled in! This is because the results from the previous pipeline execution was smartly fetched back. Critically, this was done only because Orcapod noticed that you had an identical pipeline with the same inputs and same operators/pods so that you can reuse the result as is. Should the structure of pipeline been different, the wront results would not be loaded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
