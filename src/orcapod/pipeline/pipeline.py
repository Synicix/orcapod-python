from collections import defaultdict
from collections.abc import Collection, Iterator
import json
import logging
import pickle
import sys
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Protocol, runtime_checkable

import pandas as pd

from orcapod.core import Invocation, Kernel, SyncStream
from orcapod.core.pod import FunctionPod
from orcapod.pipeline.wrappers import KernelNode, FunctionPodNode, Node

from orcapod.hashing import hash_to_hex
from orcapod.core.tracker import GraphTracker
from orcapod.hashing import ObjectHasher, ArrowHasher
from orcapod.types import TypeSpec, Tag, Packet
from orcapod.core.streams import SyncStreamFromGenerator
from orcapod.store import ArrowDataStore
from orcapod.types.registry import PacketConverter, TypeRegistry
from orcapod.types import default_registry
from orcapod.utils.stream_utils import merge_typespecs, get_typespec

logger = logging.getLogger(__name__)


class SerializationError(Exception):
    """Raised when pipeline cannot be serialized"""

    pass


class Pipeline(GraphTracker):
    """
    Enhanced pipeline that tracks operations and provides queryable views.
    Replaces the old Tracker with better persistence and view capabilities.
    """

    def __init__(self, name: str, results_store: ArrowDataStore, pipeline_store: ArrowDataStore) -> None:
        super().__init__()
        self.name = name or f"pipeline_{id(self)}"
        self.results_store = results_store
        self.pipeline_store = pipeline_store
        self.labels_to_nodes = {}

    # Core Pipeline Operations
    def save(self, path: Path | str) -> None:
        """Save complete pipeline state - named functions only"""
        path = Path(path)

        # Validate serializability first
        self._validate_serializable()

        state = {
            "name": self.name,
            "invocation_lut": self.invocation_lut,
            "metadata": {
                "created_at": time.time(),
                "python_version": sys.version_info[:2],
                "orcabridge_version": "0.1.0",  # You can make this dynamic
            },
        }

        # Atomic write
        temp_path = path.with_suffix(".tmp")
        try:
            with open(temp_path, "wb") as f:
                pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)
            temp_path.replace(path)
            logger.info(f"Pipeline '{self.name}' saved to {path}")
        except Exception:
            if temp_path.exists():
                temp_path.unlink()
            raise

    def wrap_invocation(
        self, kernel: Kernel, input_nodes: Collection[Node]
    ) -> Node:
        if isinstance(kernel, FunctionPod):
            return FunctionPodNode(kernel, input_nodes, output_store=self.results_store, tag_store=self.pipeline_store)
        return KernelNode(kernel, input_nodes, output_store=self.pipeline_store)

    def compile(self):
        import networkx as nx
        G = self.generate_graph()

        # Proposed labels for each Kernel in the graph
        # If name collides, unique name is generated by appending an index
        proposed_labels = defaultdict(list)
        node_lut = {}
        edge_lut : dict[SyncStream, Node]= {}
        for invocation in nx.topological_sort(G):
            # map streams to the new streams based on Nodes
            input_nodes = [edge_lut[stream] for stream in invocation.streams]
            new_node = self.wrap_invocation(invocation.kernel, input_nodes)

            # register the new node against the original invocation
            node_lut[invocation] = new_node
            # register the new node in the proposed labels -- if duplicates occur, will resolve later
            proposed_labels[new_node.label].append(new_node)

            for edge in G.out_edges(invocation):
                edge_lut[G.edges[edge]["stream"]] = new_node

        # resolve duplicates in proposed_labels
        labels_to_nodes = {}
        for label, nodes in proposed_labels.items():
            if len(nodes) > 1:
                # If multiple nodes have the same label, append index to make it unique
                for idx, node in enumerate(nodes):
                    node.label = f"{label}_{idx}"
                    labels_to_nodes[node.label] = node
            else:
                # If only one node, keep the original label
                nodes[0].label = label
                labels_to_nodes[label] = nodes[0]

        self.labels_to_nodes = labels_to_nodes
        return node_lut, edge_lut, proposed_labels, labels_to_nodes

    def __getattr__(self, item: str) -> Any:
        """Allow direct access to pipeline attributes"""
        if item in self.labels_to_nodes:
            return self.labels_to_nodes[item]
        raise AttributeError(f"Pipeline has no attribute '{item}'")
    
    def __dir__(self):
        # Include both regular attributes and dynamic ones
        return list(super().__dir__()) + list(self.labels_to_nodes.keys())
        

    @classmethod
    def load(cls, path: Path | str) -> "Pipeline":
        """Load complete pipeline state"""
        path = Path(path)

        with open(path, "rb") as f:
            state = pickle.load(f)

        pipeline = cls(state["name"], state["output_store"])
        pipeline.invocation_lut = state["invocation_lut"]

        logger.info(f"Pipeline '{pipeline.name}' loaded from {path}")
        return pipeline

    def _validate_serializable(self) -> None:
        """Ensure pipeline contains only serializable operations"""
        issues = []

        for operation, invocations in self.invocation_lut.items():
            # Check for lambda functions
            if hasattr(operation, "function"):
                func = getattr(operation, "function", None)
                if func and hasattr(func, "__name__") and func.__name__ == "<lambda>":
                    issues.append(f"Lambda function in {operation.__class__.__name__}")

            # Test actual serializability
            try:
                pickle.dumps(operation)
            except Exception as e:
                issues.append(f"Non-serializable operation {operation}: {e}")

        if issues:
            raise SerializationError(
                "Pipeline contains non-serializable elements:\n"
                + "\n".join(f"  - {issue}" for issue in issues)
                + "\n\nOnly named functions are supported for serialization."
            )
            

