from collections import defaultdict
from collections.abc import Collection
import logging
import pickle
import sys
import time
from pathlib import Path
from typing import Any


from orcapod.core import Invocation, Kernel, SyncStream
from orcapod.core.pod import FunctionPod
from orcapod.pipeline.wrappers import KernelNode, FunctionPodNode, Node

from orcapod.hashing import hash_to_hex
from orcapod.core.tracker import GraphTracker
from orcapod.store import ArrowDataStore

logger = logging.getLogger(__name__)


class SerializationError(Exception):
    """Raised when pipeline cannot be serialized"""

    pass


class Pipeline(GraphTracker):
    """
    Enhanced pipeline that tracks operations and provides queryable views.
    Replaces the old Tracker with better persistence and view capabilities.
    """

    def __init__(self, name: str, results_store: ArrowDataStore, pipeline_store: ArrowDataStore, auto_compile:bool=True) -> None:
        super().__init__()
        self.name = name or f"pipeline_{id(self)}"
        self.results_store = results_store
        self.pipeline_store = pipeline_store
        self.labels_to_nodes = {}
        self.auto_compile = auto_compile
        self._dirty = False
        self._ordered_nodes = []  # Track order of invocations

    # Core Pipeline Operations
    def save(self, path: Path | str) -> None:
        """Save complete pipeline state - named functions only"""
        path = Path(path)

        # Validate serializability first
        self._validate_serializable()

        state = {
            "name": self.name,
            "invocation_lut": self.invocation_lut,
            "metadata": {
                "created_at": time.time(),
                "python_version": sys.version_info[:2],
                "orcabridge_version": "0.1.0",  # You can make this dynamic
            },
        }

        # Atomic write
        temp_path = path.with_suffix(".tmp")
        try:
            with open(temp_path, "wb") as f:
                pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)
            temp_path.replace(path)
            logger.info(f"Pipeline '{self.name}' saved to {path}")
        except Exception:
            if temp_path.exists():
                temp_path.unlink()
            raise

    def record(self, invocation: Invocation) -> None:
        """
        Record an invocation in the pipeline.
        This method is called automatically by the Kernel when an operation is invoked.
        """
        super().record(invocation)
        self._dirty = True

    def wrap_invocation(
        self, kernel: Kernel, input_nodes: Collection[Node]
    ) -> Node:
        if isinstance(kernel, FunctionPod):
            return FunctionPodNode(kernel, input_nodes, output_store=self.results_store, tag_store=self.pipeline_store)
        return KernelNode(kernel, input_nodes, output_store=self.pipeline_store)

    def compile(self):
        import networkx as nx
        G = self.generate_graph()

        # Proposed labels for each Kernel in the graph
        # If name collides, unique name is generated by appending an index
        proposed_labels = defaultdict(list)
        node_lut = {}
        edge_lut : dict[SyncStream, Node]= {}
        ordered_nodes = []
        for invocation in nx.topological_sort(G):
            # map streams to the new streams based on Nodes
            input_nodes = [edge_lut[stream] for stream in invocation.streams]
            new_node = self.wrap_invocation(invocation.kernel, input_nodes)

            # register the new node against the original invocation
            node_lut[invocation] = new_node
            ordered_nodes.append(new_node)
            # register the new node in the proposed labels -- if duplicates occur, will resolve later
            proposed_labels[new_node.label].append(new_node)

            for edge in G.out_edges(invocation):
                edge_lut[G.edges[edge]["stream"]] = new_node
        
        self._ordered_nodes = ordered_nodes

        # resolve duplicates in proposed_labels
        labels_to_nodes = {}
        for label, nodes in proposed_labels.items():
            if len(nodes) > 1:
                # If multiple nodes have the same label, append index to make it unique
                for idx, node in enumerate(nodes):
                    node.label = f"{label}_{idx}"
                    labels_to_nodes[node.label] = node
            else:
                # If only one node, keep the original label
                nodes[0].label = label
                labels_to_nodes[label] = nodes[0]

        self.labels_to_nodes = labels_to_nodes
        self._dirty = False
        return node_lut, edge_lut, proposed_labels, labels_to_nodes

    def __exit__(self, exc_type, exc_val, ext_tb):
        super().__exit__(exc_type, exc_val, ext_tb)
        if self.auto_compile:
            self.compile()


    def __getattr__(self, item: str) -> Any:
        """Allow direct access to pipeline attributes"""
        if item in self.labels_to_nodes:
            return self.labels_to_nodes[item]
        raise AttributeError(f"Pipeline has no attribute '{item}'")
    
    def __dir__(self):
        # Include both regular attributes and dynamic ones
        return list(super().__dir__()) + list(self.labels_to_nodes.keys())

    def run(self, full_sync:bool=False) -> None:
        """
        Run the pipeline, compiling it if necessary.
        This method is a no-op if auto_compile is False.
        """
        if self.auto_compile and self._dirty:
            self.compile()

        # Run in topological order
        for node in self._ordered_nodes:
            if full_sync:
                node.reset_cache()
            node.flow()
            
    @classmethod
    def load(cls, path: Path | str) -> "Pipeline":
        """Load complete pipeline state"""
        path = Path(path)

        with open(path, "rb") as f:
            state = pickle.load(f)

        pipeline = cls(state["name"], state["output_store"])
        pipeline.invocation_lut = state["invocation_lut"]

        logger.info(f"Pipeline '{pipeline.name}' loaded from {path}")
        return pipeline

    def _validate_serializable(self) -> None:
        """Ensure pipeline contains only serializable operations"""
        issues = []

        for operation, invocations in self.invocation_lut.items():
            # Check for lambda functions
            if hasattr(operation, "function"):
                func = getattr(operation, "function", None)
                if func and hasattr(func, "__name__") and func.__name__ == "<lambda>":
                    issues.append(f"Lambda function in {operation.__class__.__name__}")

            # Test actual serializability
            try:
                pickle.dumps(operation)
            except Exception as e:
                issues.append(f"Non-serializable operation {operation}: {e}")

        if issues:
            raise SerializationError(
                "Pipeline contains non-serializable elements:\n"
                + "\n".join(f"  - {issue}" for issue in issues)
                + "\n\nOnly named functions are supported for serialization."
            )
            

