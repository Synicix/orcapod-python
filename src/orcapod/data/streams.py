import logging
from abc import ABC, abstractmethod
from collections.abc import AsyncIterator, Collection, Iterator, Mapping
from datetime import datetime, timezone
from itertools import repeat
from pathlib import Path
from typing import TYPE_CHECKING, Any, cast

from orcapod import contexts
from orcapod.data.base import LabeledContentIdentifiableBase
from orcapod.data.datagrams import (
    ArrowPacket,
    ArrowTag,
    DictTag,
)
from orcapod.data.system_constants import constants
from orcapod.protocols import data_protocols as dp
from orcapod.types import TypeSpec
from orcapod.utils import arrow_utils
from orcapod.utils.lazy_module import LazyModule


if TYPE_CHECKING:
    import pyarrow as pa
    import pyarrow.compute as pc
    import polars as pl
    import asyncio
else:
    pa = LazyModule("pyarrow")
    pc = LazyModule("pyarrow.compute")
    pl = LazyModule("polars")
    asyncio = LazyModule("asyncio")


# TODO: consider using this instead of making copy of dicts
# from types import MappingProxyType

logger = logging.getLogger(__name__)


def synchronous_run(async_func, *args, **kwargs):
    """
    Use existing event loop if available.

    Pros: Reuses existing loop, more efficient
    Cons: More complex, need to handle loop detection
    """
    try:
        # Check if we're already in an event loop
        loop = asyncio.get_running_loop()

        def run_in_thread():
            return asyncio.run(async_func(*args, **kwargs))

        import concurrent.futures

        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(run_in_thread)
            return future.result()
    except RuntimeError:
        # No event loop running, safe to use asyncio.run()
        return asyncio.run(async_func(*args, **kwargs))


class OperatorStreamBaseMixin:
    def join(self, other_stream: dp.Stream) -> dp.Stream:
        """
        Joins this stream with another stream, returning a new stream that contains
        the combined data from both streams.
        """
        from orcapod.data.operators import Join

        return Join()(self, other_stream)  # type: ignore[return-value]

    def semi_join(self, other_stream: dp.Stream) -> dp.Stream:
        """
        Performs a semi-join with another stream, returning a new stream that contains
        only the packets from this stream that have matching tags in the other stream.
        """
        from orcapod.data.operators import SemiJoin

        return SemiJoin()(self, other_stream)  # type: ignore[return-value]

    def map_tags(
        self, name_map: Mapping[str, str], drop_unmapped: bool = True
    ) -> dp.Stream:
        """
        Maps the tags in this stream according to the provided name_map.
        If drop_unmapped is True, any tags that are not in the name_map will be dropped.
        """
        from orcapod.data.operators import MapTags

        return MapTags(name_map, drop_unmapped)(self)  # type: ignore[return-value]

    def map_packets(
        self, name_map: Mapping[str, str], drop_unmapped: bool = True
    ) -> dp.Stream:
        """
        Maps the packets in this stream according to the provided packet_map.
        If drop_unmapped is True, any packets that are not in the packet_map will be dropped.
        """
        from orcapod.data.operators import MapPackets

        return MapPackets(name_map, drop_unmapped)(self)  # type: ignore[return-value]


class StreamBase(ABC, OperatorStreamBaseMixin, LabeledContentIdentifiableBase):
    """
    A stream is a collection of tagged-packets that are generated by an operation.
    The stream is iterable and can be used to access the packets in the stream.

    A stream has property `invocation` that is an instance of Invocation that generated the stream.
    This may be None if the stream is not generated by a kernel (i.e. directly instantiated by a user).
    """

    def __init__(
        self,
        source: dp.Kernel | None = None,
        upstreams: tuple[dp.Stream, ...] = (),
        data_context: str | contexts.DataContext | None = None,
        execution_engine: dp.ExecutionEngine | None = None,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self._source = source
        self._upstreams = upstreams
        self._last_modified: datetime | None = None
        self._set_modified_time()
        # note that this is not necessary for Stream protocol, but is provided
        # for convenience to resolve semantic types and other context-specific information
        if data_context is None and source is not None:
            # if source is provided, use its data context
            data_context = source.data_context_key
        self._data_context = contexts.resolve_context(data_context)
        self._execution_engine = execution_engine

    @property
    def substream_identities(self) -> tuple[str, ...]:
        """
        Returns the identities of the substreams that this stream is composed of.
        This is used to identify the substreams in the computational graph.
        """
        return (self.content_hash().hex(),)

    @property
    def execution_engine(self):
        """
        Returns the execution engine that is used to execute this stream.
        This is typically used to track the execution context of the stream.
        """
        return self._execution_engine

    @execution_engine.setter
    def execution_engine(self, engine: dp.ExecutionEngine | None) -> None:
        """
        Sets the execution engine for the stream.
        This is typically used to track the execution context of the stream.
        """
        self._execution_engine = engine

    def get_substream(self, substream_id: str) -> dp.Stream:
        """
        Returns the substream with the given substream_id.
        This is used to retrieve a specific substream from the stream.
        """
        if substream_id == self.substream_identities[0]:
            return self
        else:
            raise ValueError(f"Substream with ID {substream_id} not found.")

    @property
    def data_context(self) -> contexts.DataContext:
        """
        Returns the data context for the stream.
        This is used to resolve semantic types and other context-specific information.
        """
        return self._data_context

    @property
    def source(self) -> dp.Kernel | None:
        """
        The source of the stream, which is the kernel that generated the stream.
        This is typically used to track the origin of the stream in the computational graph.
        """
        return self._source

    @property
    def upstreams(self) -> tuple[dp.Stream, ...]:
        """
        The upstream streams that are used to generate this stream.
        This is typically used to track the origin of the stream in the computational graph.
        """
        return self._upstreams

    def computed_label(self) -> str | None:
        if self.source is not None:
            # use the invocation operation label
            return self.source.label
        return None

    @abstractmethod
    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]: ...

    @abstractmethod
    def types(self) -> tuple[TypeSpec, TypeSpec]: ...

    @property
    def last_modified(self) -> datetime | None:
        """
        Returns when the stream's content was last modified.
        This is used to track the time when the stream was last accessed.
        Returns None if the stream has not been accessed yet.
        """
        return self._last_modified

    @property
    def is_current(self) -> bool:
        """
        Returns whether the stream is current.
        A stream is current if the content is up-to-date with respect to its source.
        This can be used to determine if a stream with non-None last_modified is up-to-date.
        Note that for asynchronous streams, this status is not applicable and always returns False.
        """
        if self.last_modified is None:
            # If there is no last_modified timestamp, we cannot determine if the stream is current
            return False

        # check if the source kernel has been modified
        if self.source is not None and (
            self.source.last_modified is None
            or self.source.last_modified > self.last_modified
        ):
            return False

        # check if all upstreams are current
        for upstream in self.upstreams:
            if (
                not upstream.is_current
                or upstream.last_modified is None
                or upstream.last_modified > self.last_modified
            ):
                return False
        return True

    def _set_modified_time(
        self, timestamp: datetime | None = None, invalidate: bool = False
    ) -> None:
        if invalidate:
            self._last_modified = None
            return

        if timestamp is not None:
            self._last_modified = timestamp
        else:
            self._last_modified = datetime.now(timezone.utc)

    def __iter__(
        self,
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        return self.iter_packets()

    @abstractmethod
    def iter_packets(
        self,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]: ...

    @abstractmethod
    def run(
        self,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> None: ...

    @abstractmethod
    async def run_async(
        self,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> None: ...

    @abstractmethod
    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_system_tags: bool = False,
        include_content_hash: bool | str = False,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> "pa.Table": ...

    def as_df(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_system_tags: bool = False,
        include_content_hash: bool | str = False,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> "pl.DataFrame | None":
        """
        Convert the entire stream to a Polars DataFrame.
        """
        return pl.DataFrame(
            self.as_table(
                include_data_context=include_data_context,
                include_source=include_source,
                include_system_tags=include_system_tags,
                include_content_hash=include_content_hash,
                execution_engine=execution_engine,
            )
        )

    def flow(
        self, execution_engine: dp.ExecutionEngine | None = None
    ) -> Collection[tuple[dp.Tag, dp.Packet]]:
        """
        Flow everything through the stream, returning the entire collection of
        (Tag, Packet) as a collection. This will tigger any upstream computation of the stream.
        """
        return [e for e in self.iter_packets(execution_engine=execution_engine)]

    def identity_structure(self) -> Any:
        """
        Identity structure of a stream is deferred to the identity structure
        of the associated invocation, if present.
        A bare stream without invocation has no well-defined identity structure.
        Specialized stream subclasses should override this method to provide more meaningful identity structure
        """
        if self.source is not None:
            # if the stream is generated by an operation, use the identity structure from the invocation
            return self.source.identity_structure(self.upstreams)
        return super().identity_structure()


class ImmutableStream(StreamBase):
    """
    A class of stream that is constructed from immutable/constant data and does not change over time.
    Consequently, the identity of an unsourced stream should be based on the content of the stream itself.
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._data_content_identity = None

    @abstractmethod
    def data_content_identity_structure(self) -> Any:
        """
        Returns a hash of the content of the stream.
        This is used to identify the content of the stream.
        """
        ...

    def identity_structure(self) -> Any:
        if self.source is not None:
            # if the stream is generated by an operation, use the identity structure from the invocation
            return self.source.identity_structure(self.upstreams)
        # otherwise, use the content of the stream as the identity structure
        if self._data_content_identity is None:
            self._data_content_identity = self.data_content_identity_structure()
        return self._data_content_identity


class ImmutableTableStream(ImmutableStream):
    """
    An immutable stream based on a PyArrow Table.
    This stream is designed to be used with data that is already in a tabular format,
    such as data loaded from a file or database. The columns to be treated as tags are
    specified at initialization, and the rest of the columns are treated as packets.
    The stream is immutable, meaning that once it is created, it cannot be modified.
    This is useful for ensuring that the data in the stream remains consistent and unchanging.

    The types of the tag and packet columns are inferred from the PyArrow Table schema.
    """

    def __init__(
        self,
        table: "pa.Table",
        tag_columns: Collection[str] = (),
        source_info: dict[str, str | None] | None = None,
        source: dp.Kernel | None = None,
        upstreams: tuple[dp.Stream, ...] = (),
        **kwargs,
    ) -> None:
        super().__init__(source=source, upstreams=upstreams, **kwargs)

        data_table, data_context_table = arrow_utils.split_by_column_groups(
            table, [constants.CONTEXT_KEY]
        )
        if data_table is None:
            # TODO: provide better error message
            raise ValueError(
                "Table must contain at least one column to be used as a stream."
            )

        if data_context_table is None:
            data_context_table = pa.table(
                {constants.CONTEXT_KEY: pa.nulls(len(data_table), pa.large_string())}
            )

        prefix_info = {constants.SOURCE_PREFIX: source_info}

        # determine tag columns first and then exclude any source info
        self._tag_columns = tuple(c for c in tag_columns if c in table.column_names)
        self._system_tag_columns = tuple(
            c for c in table.column_names if c.startswith(constants.SYSTEM_TAG_PREFIX)
        )
        self._all_tag_columns = self._tag_columns + self._system_tag_columns
        if delta := set(tag_columns) - set(self._tag_columns):
            raise ValueError(
                f"Specified tag columns {delta} are not present in the table."
            )
        table, prefix_tables = arrow_utils.prepare_prefixed_columns(
            table,
            prefix_info,
            exclude_columns=self._all_tag_columns,
        )
        # now table should only contain tag columns and packet columns
        self._packet_columns = tuple(
            c for c in table.column_names if c not in self._all_tag_columns
        )
        self._table = table
        self._source_info_table = prefix_tables[constants.SOURCE_PREFIX]
        self._data_context_table = data_context_table

        if len(self._packet_columns) == 0:
            raise ValueError(
                "No packet columns found in the table. At least one packet column is required."
            )

        tag_schema = pa.schema(
            f for f in self._table.schema if f.name in self._tag_columns
        )
        system_tag_schema = pa.schema(
            f for f in self._table.schema if f.name in self._system_tag_columns
        )
        all_tag_schema = arrow_utils.join_arrow_schemas(tag_schema, system_tag_schema)
        packet_schema = pa.schema(
            f for f in self._table.schema if f.name in self._packet_columns
        )

        self._tag_schema = tag_schema
        self._system_tag_schema = system_tag_schema
        self._all_tag_schema = all_tag_schema
        self._packet_schema = packet_schema
        # self._tag_converter = SemanticConverter.from_semantic_schema(
        #     schemas.SemanticSchema.from_arrow_schema(
        #         tag_schema, self._data_context.semantic_type_registry
        #     )
        # )
        # self._packet_converter = SemanticConverter.from_semantic_schema(
        #     schemas.SemanticSchema.from_arrow_schema(
        #         packet_schema, self._data_context.semantic_type_registry
        #     )
        # )

        self._cached_elements: list[tuple[dp.Tag, ArrowPacket]] | None = None
        self._set_modified_time()  # set modified time to now

    def data_content_identity_structure(self) -> Any:
        """
        Returns a hash of the content of the stream.
        This is used to identify the content of the stream.
        """
        table_hash = self._data_context.arrow_hasher.hash_table(
            self.as_table(
                include_data_context=True, include_source=True, include_system_tags=True
            ),
        )
        return (
            self.__class__.__name__,
            table_hash,
            self._tag_columns,
        )

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        return self._tag_columns, self._packet_columns

    def types(self) -> tuple[dict[str, type], dict[str, type]]:
        """
        Returns the types of the tag and packet columns in the stream.
        This is useful for accessing the types of the columns in the stream.
        """
        # TODO: consider using MappingProxyType to avoid copying the dicts
        converter = self._data_context.type_converter
        return (
            converter.arrow_schema_to_python_schema(self._tag_schema),
            converter.arrow_schema_to_python_schema(self._packet_schema),
        )

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_system_tags: bool = False,
        include_content_hash: bool | str = False,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> "pa.Table":
        """
        Returns the underlying table representation of the stream.
        This is useful for converting the stream to a table format.
        """
        output_table = self._table
        if include_content_hash:
            hash_column_name = (
                "_content_hash"
                if include_content_hash is True
                else include_content_hash
            )
            content_hashes = [
                packet.content_hash() for _, packet in self.iter_packets()
            ]
            output_table = output_table.append_column(
                hash_column_name, pa.array(content_hashes, type=pa.large_string())
            )
        if not include_system_tags:
            # Check in original implementation
            output_table = output_table.drop_columns(list(self._system_tag_columns))
        table_stack = (output_table,)
        if include_data_context:
            table_stack += (self._data_context_table,)
        if include_source:
            table_stack += (self._source_info_table,)

        return arrow_utils.hstack_tables(*table_stack)

    def clear_cache(self) -> None:
        """
        Resets the cached elements of the stream.
        This is useful for re-iterating over the stream.
        """
        self._cached_elements = None

    def iter_packets(
        self, execution_engine: dp.ExecutionEngine | None = None
    ) -> Iterator[tuple[dp.Tag, ArrowPacket]]:
        """
        Iterates over the packets in the stream.
        Each packet is represented as a tuple of (Tag, Packet).
        """
        # TODO: make it work with table batch stream
        if self._cached_elements is None:
            self._cached_elements = []
            tag_present = len(self._all_tag_columns) > 0
            if tag_present:
                tags = self._table.select(self._all_tag_columns)
                tag_batches = tags.to_batches()
            else:
                tag_batches = repeat(DictTag({}))

            # TODO: come back and clean up this logic

            packets = self._table.select(self._packet_columns)
            for tag_batch, packet_batch in zip(tag_batches, packets.to_batches()):
                for i in range(len(packet_batch)):
                    if tag_present:
                        tag = ArrowTag(
                            tag_batch.slice(i, 1),  # type: ignore
                            data_context=self._data_context,
                        )

                    else:
                        tag = cast(DictTag, tag_batch)

                    self._cached_elements.append(
                        (
                            tag,
                            ArrowPacket(
                                packet_batch.slice(i, 1),
                                source_info=self._source_info_table.slice(
                                    i, 1
                                ).to_pylist()[0],
                                data_context=self._data_context,
                            ),
                        )
                    )
        yield from self._cached_elements

    def run(self, execution_engine: dp.ExecutionEngine | None = None) -> None:
        """
        Runs the stream, which in this case is a no-op since the stream is immutable.
        This is typically used to trigger any upstream computation of the stream.
        """
        # No-op for immutable streams
        pass

    async def run_async(
        self, execution_engine: dp.ExecutionEngine | None = None
    ) -> None:
        """
        Runs the stream asynchronously, which in this case is a no-op since the stream is immutable.
        This is typically used to trigger any upstream computation of the stream.
        """
        # No-op for immutable streams
        pass

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}(table={self._table.column_names}, "
            f"tag_columns={self._tag_columns})"
        )


class KernelStream(StreamBase):
    """
    Recomputable stream that wraps a stream produced by a kernel to provide
    an abstraction over the stream, taking the stream's source and upstreams as the basis of
    recomputing the stream.

    This stream is used to represent the output of a kernel invocation.
    """

    def __init__(
        self,
        output_stream: dp.Stream | None = None,
        source: dp.Kernel | None = None,
        upstreams: tuple[
            dp.Stream, ...
        ] = (),  # if provided, this will override the upstreams of the output_stream
        **kwargs,
    ) -> None:
        if (output_stream is None or output_stream.source is None) and source is None:
            raise ValueError(
                "Either output_stream must have a kernel assigned to it or source must be provided in order to be recomputable."
            )
        if source is None:
            if output_stream is None or output_stream.source is None:
                raise ValueError(
                    "Either output_stream must have a kernel assigned to it or source must be provided in order to be recomputable."
                )
            source = output_stream.source
            upstreams = upstreams or output_stream.upstreams

        super().__init__(source=source, upstreams=upstreams, **kwargs)
        self.kernel = source
        self._cached_stream = output_stream

    def clear_cache(self) -> None:
        """
        Clears the cached stream.
        This is useful for re-processing the stream with the same kernel.
        """
        self._cached_stream = None
        self._set_modified_time(invalidate=True)

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        tag_types, packet_types = self.kernel.output_types(*self.upstreams)
        return tuple(tag_types.keys()), tuple(packet_types.keys())

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        """
        Returns the types of the tag and packet columns in the stream.
        This is useful for accessing the types of the columns in the stream.
        """
        return self.kernel.output_types(*self.upstreams)

    @property
    def is_current(self) -> bool:
        if self._cached_stream is None or not super().is_current:
            status = self.refresh()
            if not status:  # if it failed to update for whatever reason
                return False
        return True

    def refresh(self, force: bool = False) -> bool:
        updated = False
        if force or (self._cached_stream is not None and not super().is_current):
            self.clear_cache()

        if self._cached_stream is None:
            assert self.source is not None, (
                "Stream source must be set to recompute the stream."
            )
            self._cached_stream = self.source.forward(*self.upstreams)
            self._set_modified_time()
            updated = True

        if self._cached_stream is None:
            # TODO: use beter error type
            raise ValueError(
                "Stream could not be updated. Ensure that the source is valid and upstreams are correct."
            )

        return updated

    def invalidate(self) -> None:
        """
        Invalidate the stream, marking it as needing recomputation.
        This will clear the cached stream and set the last modified time to None.
        """
        self.clear_cache()
        self._set_modified_time(invalidate=True)

    @property
    def last_modified(self) -> datetime | None:
        if self._cached_stream is None:
            return None
        return self._cached_stream.last_modified

    def run(self, execution_engine: dp.ExecutionEngine | None = None) -> None:
        self.refresh()
        assert self._cached_stream is not None, (
            "Stream has not been updated or is empty."
        )
        self._cached_stream.run(execution_engine=execution_engine)

    async def run_async(
        self, execution_engine: dp.ExecutionEngine | None = None
    ) -> None:
        self.refresh()
        assert self._cached_stream is not None, (
            "Stream has not been updated or is empty."
        )
        await self._cached_stream.run_async(execution_engine=execution_engine)

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_system_tags: bool = False,
        include_content_hash: bool | str = False,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> "pa.Table":
        self.refresh()
        assert self._cached_stream is not None, (
            "Stream has not been updated or is empty."
        )
        return self._cached_stream.as_table(
            include_data_context=include_data_context,
            include_source=include_source,
            include_system_tags=include_system_tags,
            include_content_hash=include_content_hash,
            execution_engine=execution_engine,
        )

    def iter_packets(
        self,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        self.refresh()
        assert self._cached_stream is not None, (
            "Stream has not been updated or is empty."
        )
        return self._cached_stream.iter_packets(execution_engine=execution_engine)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(kernel={self.source}, upstreams={self.upstreams})"


class LazyPodResultStream(StreamBase):
    """
    A fixed stream that lazily processes packets from a prepared input stream.
    This is what Pod.process() returns - it's static/fixed but efficient.
    """

    def __init__(self, pod: dp.Pod, prepared_stream: dp.Stream, **kwargs):
        super().__init__(source=pod, upstreams=(prepared_stream,), **kwargs)
        self.pod = pod
        self.prepared_stream = prepared_stream
        self._set_modified_time()  # set modified time to when we obtain the iterator
        # capture the immutable iterator from the prepared stream
        self._prepared_stream_iterator = prepared_stream.iter_packets()

        # Packet-level caching (from your PodStream)
        self._cached_output_packets: dict[int, tuple[dp.Tag, dp.Packet | None]] = {}
        self._cached_output_table: pa.Table | None = None
        self._cached_content_hash_column: pa.Array | None = None

    def iter_packets(
        self, execution_engine: dp.ExecutionEngine | None = None
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        if self._prepared_stream_iterator is not None:
            for i, (tag, packet) in enumerate(self._prepared_stream_iterator):
                if i in self._cached_output_packets:
                    # Use cached result
                    tag, packet = self._cached_output_packets[i]
                    if packet is not None:
                        yield tag, packet
                else:
                    # Process packet
                    processed = self.pod.call(
                        tag, packet, execution_engine=execution_engine
                    )
                    if processed is not None:
                        # Update shared cache for future iterators (optimization)
                        self._cached_output_packets[i] = processed
                        tag, packet = processed
                        if packet is not None:
                            yield tag, packet

            # Mark completion by releasing the iterator
            self._prepared_stream_iterator = None
        else:
            # Yield from snapshot of complete cache
            for i in range(len(self._cached_output_packets)):
                tag, packet = self._cached_output_packets[i]
                if packet is not None:
                    yield tag, packet

    async def run_async(
        self, execution_engine: dp.ExecutionEngine | None = None
    ) -> None:
        if self._prepared_stream_iterator is not None:
            pending_call_lut = {}
            for i, (tag, packet) in enumerate(self._prepared_stream_iterator):
                if i not in self._cached_output_packets:
                    # Process packet
                    pending_call_lut[i] = self.pod.async_call(
                        tag, packet, execution_engine=execution_engine
                    )

            indices = list(pending_call_lut.keys())
            pending_calls = [pending_call_lut[i] for i in indices]

            results = await asyncio.gather(*pending_calls)
            for i, result in zip(indices, results):
                self._cached_output_packets[i] = result

            # Mark completion by releasing the iterator
            self._prepared_stream_iterator = None

    def run(
        self,
        execution_engine: dp.ExecutionEngine | None = None,
        try_async_backend: bool = True,
    ) -> None:
        if try_async_backend:
            # Use async run if requested
            try:
                return synchronous_run(
                    self.run_async, execution_engine=execution_engine
                )
            except RuntimeError as e:
                logger.warning(
                    "Failed to run async stream synchronously, falling back to sync run: %s",
                    e,
                )
                # Fallback to synchronous run
        self.flow(execution_engine=execution_engine)

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """

        tag_keys, _ = self.prepared_stream.keys()
        packet_keys = tuple(self.pod.output_packet_types().keys())
        return tag_keys, packet_keys

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        tag_typespec, _ = self.prepared_stream.types()
        # TODO: check if copying can be avoided
        packet_typespec = dict(self.pod.output_packet_types())
        return tag_typespec, packet_typespec

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_system_tags: bool = False,
        include_content_hash: bool | str = False,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> "pa.Table":
        if self._cached_output_table is None:
            all_tags = []
            all_packets = []
            tag_schema, packet_schema = None, None
            for tag, packet in self.iter_packets(execution_engine=execution_engine):
                if tag_schema is None:
                    tag_schema = tag.arrow_schema(include_system_tags=True)
                if packet_schema is None:
                    packet_schema = packet.arrow_schema(
                        include_context=True,
                        include_source=True,
                    )
                all_tags.append(tag.as_dict(include_system_tags=True))
                # FIXME: using in the pinch conversion to str from path
                # replace with an appropriate semantic converter-based approach!
                dict_patcket = packet.as_dict(include_context=True, include_source=True)
                for k, v in dict_patcket.items():
                    if isinstance(v, Path):
                        dict_patcket[k] = str(v)
                all_packets.append(dict_patcket)

            # TODO: re-verify the implemetation of this conversion
            converter = self._data_context.type_converter

            struct_packets = converter.python_dicts_to_struct_dicts(all_packets)
            all_tags_as_tables: pa.Table = pa.Table.from_pylist(
                all_tags, schema=tag_schema
            )
            all_packets_as_tables: pa.Table = pa.Table.from_pylist(
                struct_packets, schema=packet_schema
            )

            self._cached_output_table = arrow_utils.hstack_tables(
                all_tags_as_tables, all_packets_as_tables
            )
        assert self._cached_output_table is not None, (
            "_cached_output_table should not be None here."
        )

        drop_columns = []
        if not include_system_tags:
            # TODO: get system tags more effiicently
            drop_columns.extend(
                [
                    c
                    for c in self._cached_output_table.column_names
                    if c.startswith(constants.SYSTEM_TAG_PREFIX)
                ]
            )
        if not include_source:
            drop_columns.extend(f"{constants.SOURCE_PREFIX}{c}" for c in self.keys()[1])
        if not include_data_context:
            drop_columns.append(constants.CONTEXT_KEY)

        output_table = self._cached_output_table.drop(drop_columns)

        # lazily prepare content hash column if requested
        if include_content_hash:
            if self._cached_content_hash_column is None:
                content_hashes = []
                # TODO: verify that order will be preserved
                for tag, packet in self.iter_packets():
                    content_hashes.append(packet.content_hash())
                self._cached_content_hash_column = pa.array(
                    content_hashes, type=pa.large_string()
                )
            assert self._cached_content_hash_column is not None, (
                "_cached_content_hash_column should not be None here."
            )
            hash_column_name = (
                "_content_hash"
                if include_content_hash is True
                else include_content_hash
            )
            output_table = output_table.append_column(
                hash_column_name, self._cached_content_hash_column
            )
        return output_table


class EfficientPodResultStream(StreamBase):
    """
    A fixed stream that lazily processes packets from a prepared input stream.
    This is what Pod.process() returns - it's static/fixed but efficient.
    """

    # TODO: define interface for storage or pod storage
    def __init__(self, pod: dp.CachedPod, input_stream: dp.Stream, **kwargs):
        super().__init__(source=pod, upstreams=(input_stream,), **kwargs)
        self.pod = pod
        self.input_stream = input_stream
        self._set_modified_time()  # set modified time to when we obtain the iterator
        # capture the immutable iterator from the input stream

        self._prepared_stream_iterator = input_stream.iter_packets()

        # Packet-level caching (from your PodStream)
        self._cached_output_packets: list[tuple[dp.Tag, dp.Packet | None]] | None = None
        self._cached_output_table: pa.Table | None = None
        self._cached_content_hash_column: pa.Array | None = None

    async def run_async(
        self, execution_engine: dp.ExecutionEngine | None = None
    ) -> None:
        """
        Runs the stream, processing the input stream and preparing the output stream.
        This is typically called before iterating over the packets.
        """
        if self._cached_output_packets is None:
            cached_results = []

            # identify all entries in the input stream for which we still have not computed packets
            target_entries = self.input_stream.as_table(
                include_content_hash=constants.INPUT_PACKET_HASH
            )
            existing_entries = self.pod.get_all_records(include_system_columns=True)
            if existing_entries is None or existing_entries.num_rows == 0:
                missing = target_entries.drop_columns([constants.INPUT_PACKET_HASH])
                existing = None
            else:
                # missing = target_entries.join(
                #     existing_entries,
                #     keys=[constants.INPUT_PACKET_HASH],
                #     join_type="left anti",
                # )
                # Single join that gives you both missing and existing
                # More efficient - only bring the key column from existing_entries
                # .select([constants.INPUT_PACKET_HASH]).append_column(
                #     "_exists", pa.array([True] * len(existing_entries))
                # ),
                all_results = target_entries.join(
                    existing_entries.append_column(
                        "_exists", pa.array([True] * len(existing_entries))
                    ),
                    keys=[constants.INPUT_PACKET_HASH],
                    join_type="left outer",
                    right_suffix="_right",
                )
                # grab all columns from target_entries first
                missing = (
                    all_results.filter(pc.is_null(pc.field("_exists")))
                    .select(target_entries.column_names)
                    .drop_columns([constants.INPUT_PACKET_HASH])
                )

                existing = (
                    all_results.filter(pc.is_valid(pc.field("_exists")))
                    .drop_columns(target_entries.column_names)
                    .drop_columns(["_exists"])
                )
                renamed = [
                    c.removesuffix("_right") if c.endswith("_right") else c
                    for c in existing.column_names
                ]
                existing = existing.rename_columns(renamed)

            tag_keys = self.input_stream.keys()[0]

            if existing is not None and existing.num_rows > 0:
                # If there are existing entries, we can cache them
                existing_stream = ImmutableTableStream(existing, tag_columns=tag_keys)
                for tag, packet in existing_stream.iter_packets():
                    cached_results.append((tag, packet))

            pending_calls = []
            if missing is not None and missing.num_rows > 0:
                for tag, packet in ImmutableTableStream(missing, tag_columns=tag_keys):
                    # Since these packets are known to be missing, skip the cache lookup
                    pending = self.pod.async_call(
                        tag,
                        packet,
                        skip_cache_lookup=True,
                        execution_engine=execution_engine,
                    )
                    pending_calls.append(pending)
            import asyncio

            completed_calls = await asyncio.gather(*pending_calls)
            for result in completed_calls:
                cached_results.append(result)

            self._cached_output_packets = cached_results
            self._set_modified_time()

    def run(
        self,
        execution_engine: dp.ExecutionEngine | None = None,
        try_async_backend: bool = True,
    ) -> None:
        if try_async_backend:
            # Use async run if requested
            try:
                return synchronous_run(
                    self.run_async, execution_engine=execution_engine
                )
            except RuntimeError as e:
                logger.warning(
                    "Failed to run async stream synchronously, falling back to sync run: %s",
                    e,
                )
                # Fallback to synchronous run
        self.flow(execution_engine=execution_engine)

    def iter_packets(
        self, execution_engine: dp.ExecutionEngine | None = None
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        """
        Processes the input stream and prepares the output stream.
        This is typically called before iterating over the packets.
        """
        if self._cached_output_packets is None:
            cached_results = []

            # identify all entries in the input stream for which we still have not computed packets
            target_entries = self.input_stream.as_table(
                include_content_hash=constants.INPUT_PACKET_HASH,
                execution_engine=execution_engine,
            )
            existing_entries = self.pod.get_all_records(include_system_columns=True)
            if existing_entries is None or existing_entries.num_rows == 0:
                missing = target_entries.drop_columns([constants.INPUT_PACKET_HASH])
                existing = None
            else:
                # missing = target_entries.join(
                #     existing_entries,
                #     keys=[constants.INPUT_PACKET_HASH],
                #     join_type="left anti",
                # )
                # Single join that gives you both missing and existing
                # More efficient - only bring the key column from existing_entries
                # .select([constants.INPUT_PACKET_HASH]).append_column(
                #     "_exists", pa.array([True] * len(existing_entries))
                # ),
                all_results = target_entries.join(
                    existing_entries.append_column(
                        "_exists", pa.array([True] * len(existing_entries))
                    ),
                    keys=[constants.INPUT_PACKET_HASH],
                    join_type="left outer",
                    right_suffix="_right",
                )
                # grab all columns from target_entries first
                missing = (
                    all_results.filter(pc.is_null(pc.field("_exists")))
                    .select(target_entries.column_names)
                    .drop_columns([constants.INPUT_PACKET_HASH])
                )

                existing = (
                    all_results.filter(pc.is_valid(pc.field("_exists")))
                    .drop_columns(target_entries.column_names)
                    .drop_columns(["_exists"])
                )
                renamed = [
                    c.removesuffix("_right") if c.endswith("_right") else c
                    for c in existing.column_names
                ]
                existing = existing.rename_columns(renamed)

            tag_keys = self.input_stream.keys()[0]

            if existing is not None and existing.num_rows > 0:
                # If there are existing entries, we can cache them
                existing_stream = ImmutableTableStream(existing, tag_columns=tag_keys)
                for tag, packet in existing_stream.iter_packets():
                    cached_results.append((tag, packet))
                    yield tag, packet

            if missing is not None and missing.num_rows > 0:
                for tag, packet in ImmutableTableStream(missing, tag_columns=tag_keys):
                    # Since these packets are known to be missing, skip the cache lookup
                    tag, packet = self.pod.call(
                        tag,
                        packet,
                        skip_cache_lookup=True,
                        execution_engine=execution_engine,
                    )
                    cached_results.append((tag, packet))
                    if packet is not None:
                        yield tag, packet

            self._cached_output_packets = cached_results
            self._set_modified_time()
        else:
            for tag, packet in self._cached_output_packets:
                if packet is not None:
                    yield tag, packet

    # def iter_packets(self) -> Iterator[tuple[dp.Tag, dp.Packet]]:
    #     if self._prepared_stream_iterator is not None:
    #         for i, (tag, packet) in enumerate(self._prepared_stream_iterator):
    #             if i in self._cached_output_packets:
    #                 # Use cached result
    #                 tag, packet = self._cached_output_packets[i]
    #                 if packet is not None:
    #                     yield tag, packet
    #             else:
    #                 # Process packet
    #                 processed = self.pod.call(tag, packet)
    #                 if processed is not None:
    #                     # Update shared cache for future iterators (optimization)
    #                     self._cached_output_packets[i] = processed
    #                     tag, packet = processed
    #                     if packet is not None:
    #                         yield tag, packet

    #         # Mark completion by releasing the iterator
    #         self._prepared_stream_iterator = None
    #     else:
    #         # Yield from snapshot of complete cache
    #         for i in range(len(self._cached_output_packets)):
    #             tag, packet = self._cached_output_packets[i]
    #             if packet is not None:
    #                 yield tag, packet

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """

        tag_keys, _ = self.input_stream.keys()
        packet_keys = tuple(self.pod.output_packet_types().keys())
        return tag_keys, packet_keys

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        tag_typespec, _ = self.input_stream.types()
        # TODO: check if copying can be avoided
        packet_typespec = dict(self.pod.output_packet_types())
        return tag_typespec, packet_typespec

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_system_tags: bool = False,
        include_content_hash: bool | str = False,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> "pa.Table":
        if self._cached_output_table is None:
            all_tags = []
            all_packets = []
            tag_schema, packet_schema = None, None
            for tag, packet in self.iter_packets(execution_engine=execution_engine):
                if tag_schema is None:
                    tag_schema = tag.arrow_schema(include_system_tags=True)
                if packet_schema is None:
                    packet_schema = packet.arrow_schema(
                        include_context=True,
                        include_source=True,
                    )
                all_tags.append(tag.as_dict(include_system_tags=True))
                # FIXME: using in the pinch conversion to str from path
                # replace with an appropriate semantic converter-based approach!
                dict_patcket = packet.as_dict(include_context=True, include_source=True)
                for k, v in dict_patcket.items():
                    if isinstance(v, Path):
                        dict_patcket[k] = str(v)
                all_packets.append(dict_patcket)

            converter = self._data_context.type_converter

            struct_packets = converter.python_dicts_to_struct_dicts(all_packets)
            all_tags_as_tables: pa.Table = pa.Table.from_pylist(
                all_tags, schema=tag_schema
            )
            all_packets_as_tables: pa.Table = pa.Table.from_pylist(
                struct_packets, schema=packet_schema
            )

            self._cached_output_table = arrow_utils.hstack_tables(
                all_tags_as_tables, all_packets_as_tables
            )
        assert self._cached_output_table is not None, (
            "_cached_output_table should not be None here."
        )

        drop_columns = []
        if not include_source:
            drop_columns.extend(f"{constants.SOURCE_PREFIX}{c}" for c in self.keys()[1])
        if not include_data_context:
            drop_columns.append(constants.CONTEXT_KEY)
        if not include_system_tags:
            # TODO: come up with a more efficient approach
            drop_columns.extend(
                [
                    c
                    for c in self._cached_output_table.column_names
                    if c.startswith(constants.SYSTEM_TAG_PREFIX)
                ]
            )

        output_table = self._cached_output_table.drop_columns(drop_columns)

        # lazily prepare content hash column if requested
        if include_content_hash:
            if self._cached_content_hash_column is None:
                content_hashes = []
                for tag, packet in self.iter_packets(execution_engine=execution_engine):
                    content_hashes.append(packet.content_hash())
                self._cached_content_hash_column = pa.array(
                    content_hashes, type=pa.large_string()
                )
            assert self._cached_content_hash_column is not None, (
                "_cached_content_hash_column should not be None here."
            )
            hash_column_name = (
                "_content_hash"
                if include_content_hash is True
                else include_content_hash
            )
            output_table = output_table.append_column(
                hash_column_name, self._cached_content_hash_column
            )
        return output_table


class WrappedStream(StreamBase):
    def __init__(
        self,
        stream: dp.Stream,
        source: dp.Kernel,
        input_streams: tuple[dp.Stream, ...],
        label: str | None = None,
        **kwargs,
    ) -> None:
        super().__init__(source=source, upstreams=input_streams, label=label, **kwargs)
        self._stream = stream

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        return self._stream.keys()

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        """
        Returns the types of the tag and packet columns in the stream.
        This is useful for accessing the types of the columns in the stream.
        """
        return self._stream.types()

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_system_tags: bool = False,
        include_content_hash: bool | str = False,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> "pa.Table":
        """
        Returns the underlying table representation of the stream.
        This is useful for converting the stream to a table format.
        """
        return self._stream.as_table(
            include_data_context=include_data_context,
            include_source=include_source,
            include_system_tags=include_system_tags,
            include_content_hash=include_content_hash,
            execution_engine=execution_engine,
        )

    def iter_packets(
        self,
        execution_engine: dp.ExecutionEngine | None = None,
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        """
        Iterates over the packets in the stream.
        Each packet is represented as a tuple of (Tag, Packet).
        """
        return self._stream.iter_packets(execution_engine=execution_engine)

    def identity_structure(self) -> Any:
        return self._stream.identity_structure()
