from orcapod.protocols import data_protocols as dp
from orcapod.types import SemanticTypeRegistry, default_registry, schemas, TypeSpec
from orcapod.data.datagrams import ArrowPacket, ArrowTag, SemanticConverter
from orcapod.data.base import LabeledContentIdentifiableBase
import pyarrow as pa
from collections.abc import Iterator, Collection
from abc import ABC, abstractmethod
from datetime import timezone, datetime
from typing import Any, Literal
import logging
import warnings

# TODO: consider using this instead of making copy of dicts
# from types import MappingProxyType

logger = logging.getLogger(__name__)


class StreamBase(ABC, LabeledContentIdentifiableBase):
    """
    A stream is a collection of tagged-packets that are generated by an operation.
    The stream is iterable and can be used to access the packets in the stream.

    A stream has property `invocation` that is an instance of Invocation that generated the stream.
    This may be None if the stream is not generated by a kernel (i.e. directly instantiated by a user).
    """

    def __init__(
        self,
        source: dp.Kernel | None = None,
        upstreams: tuple[dp.Stream, ...] = (),
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self._source = source
        self._upstreams = upstreams
        self._last_modified: datetime | None = None
        self._set_modified_time()

    @property
    def source(self) -> dp.Kernel | None:
        """
        The source of the stream, which is the kernel that generated the stream.
        This is typically used to track the origin of the stream in the computational graph.
        """
        return self._source

    @property
    def upstreams(self) -> tuple[dp.Stream, ...]:
        """
        The upstream streams that are used to generate this stream.
        This is typically used to track the origin of the stream in the computational graph.
        """
        return self._upstreams

    def computed_label(self) -> str | None:
        if self.source is not None:
            # use the invocation operation label
            return self.source.label
        return None

    @abstractmethod
    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]: ...

    @abstractmethod
    def types(self) -> tuple[TypeSpec, TypeSpec]: ...

    @property
    def last_modified(self) -> datetime | None:
        """
        Returns when the stream's content was last modified.
        This is used to track the time when the stream was last accessed.
        Returns None if the stream has not been accessed yet.
        """
        return self._last_modified

    @property
    def is_current(self) -> bool:
        """
        Returns whether the stream is current.
        A stream is current if the content is up-to-date with respect to its source.
        This can be used to determine if a stream with non-None last_modified is up-to-date.
        Note that for asynchronous streams, this status is not applicable and always returns False.
        """
        if self.last_modified is None:
            # If there is no last_modified timestamp, we cannot determine if the stream is current
            return False

        for upstream in self.upstreams:
            if (
                not upstream.is_current
                or upstream.last_modified is None
                or upstream.last_modified > self.last_modified
            ):
                return False
        return True

    def _set_modified_time(
        self, timestamp: datetime | None = None, invalidate: bool = False
    ) -> None:
        if invalidate:
            self._last_modified = None
            return

        if timestamp is not None:
            self._last_modified = timestamp
        else:
            self._last_modified = datetime.now(timezone.utc)

    def __iter__(
        self,
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        return self.iter_packets()

    @abstractmethod
    def iter_packets(
        self,
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]: ...

    @abstractmethod
    def as_table(self) -> pa.Table: ...

    def flow(self) -> Collection[tuple[dp.Tag, dp.Packet]]:
        """
        Flow everything through the stream, returning the entire collection of
        (Tag, Packet) as a collection. This will tigger any upstream computation of the stream.
        """
        return [e for e in self]

    # --------------------- Recursive methods ---------------------------
    # These methods form a step in the multi-class recursive invocation that follows the pattern of
    # Stream -> Invocation -> Kernel -> Stream ... -> Invocation -> Kernel
    # Most of the method logic would be found in Kernel's implementation of the method with
    # Stream and Invocation simply serving as recursive steps

    def identity_structure(self) -> Any:
        """
        Identity structure of a stream is deferred to the identity structure
        of the associated invocation, if present.
        A bare stream without invocation has no well-defined identity structure.
        Specialized stream subclasses should override this method to provide more meaningful identity structure
        """
        if self.source is not None:
            # if the stream is generated by an operation, use the identity structure from the invocation
            return self.source.identity_structure(*self.upstreams)
        return super().identity_structure()


class KernelStream(StreamBase):
    """
    Recomputable stream that wraps a streams produced by a kernel to provide
    an abstraction over the stream, taking the stream's source and upstreams as the basis of
    recomputing the stream.

    This stream is used to represent the output of a kernel invocation.
    """

    def __init__(
        self,
        output_stream: dp.Stream | None = None,
        source: dp.Kernel | None = None,
        upstreams: tuple[
            dp.Stream, ...
        ] = (),  # if provided, this will override the upstreams of the output_stream
        **kwargs,
    ) -> None:
        if (output_stream is None or output_stream.source is None) and source is None:
            raise ValueError(
                "Either output_stream must have a kernel assigned to it or source must be provided in order to be recomputable."
            )
        if source is None:
            if output_stream is None or output_stream.source is None:
                raise ValueError(
                    "Either output_stream must have a kernel assigned to it or source must be provided in order to be recomputable."
                )
            source = output_stream.source
            upstreams = upstreams or output_stream.upstreams

        super().__init__(source=source, upstreams=upstreams, **kwargs)
        self._cached_stream = output_stream

    def clear_cache(self) -> None:
        """
        Clears the cached stream.
        This is useful for re-processing the stream with the same kernel.
        """
        self._cached_stream = None
        self._set_modified_time(invalidate=True)

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        self.refresh()
        assert self._cached_stream is not None, (
            "_cached_stream should not be None here."
        )
        return self._cached_stream.keys()

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        """
        Returns the types of the tag and packet columns in the stream.
        This is useful for accessing the types of the columns in the stream.
        """
        self.refresh()
        assert self._cached_stream is not None, (
            "_cached_stream should not be None here."
        )
        return self._cached_stream.types()

    @property
    def is_current(self) -> bool:
        if self._cached_stream is None or not super().is_current:
            status = self.refresh()
            if not status:  # if it failed to update for whatever reason
                return False
        return True

    def refresh(self, force: bool = False) -> bool:
        updated = False
        if force or (self._cached_stream is not None and not super().is_current):
            self.clear_cache()

        if self._cached_stream is None:
            assert self.source is not None, (
                "Stream source must be set to recompute the stream."
            )
            self._cached_stream = self.source.forward(*self.upstreams)
            self._set_modified_time()
            updated = True

        if self._cached_stream is None:
            # TODO: use beter error type
            raise ValueError(
                "Stream could not be updated. Ensure that the source is valid and upstreams are correct."
            )

        return updated

    def invalidate(self) -> None:
        """
        Invalidate the stream, marking it as needing recomputation.
        This will clear the cached stream and set the last modified time to None.
        """
        self.clear_cache()
        self._set_modified_time(invalidate=True)

    @property
    def last_modified(self) -> datetime | None:
        if self._cached_stream is None:
            return None
        return self._cached_stream.last_modified

    def as_table(self) -> pa.Table:
        self.refresh()
        assert self._cached_stream is not None, (
            "Stream has not been updated or is empty."
        )
        return self._cached_stream.as_table()

    def iter_packets(self) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        self.refresh()
        assert self._cached_stream is not None, (
            "Stream has not been updated or is empty."
        )
        yield from self._cached_stream.iter_packets()

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(kernel={self.source}, upstreams={self.upstreams})"


class ImmutableTableStream(StreamBase):
    """
    An immutable stream based on a PyArrow Table.
    This stream is designed to be used with data that is already in a tabular format,
    such as data loaded from a file or database. The columns to be treated as tags are
    specified at initialization, and the rest of the columns are treated as packets.
    The stream is immutable, meaning that once it is created, it cannot be modified.
    This is useful for ensuring that the data in the stream remains consistent and unchanging.

    The types of the tag and packet columns are inferred from the PyArrow Table schema.
    """

    def __init__(
        self,
        table: pa.Table,
        tag_columns: Collection[str] = (),
        source: dp.Kernel | None = None,
        upstreams: tuple[dp.Stream, ...] = (),
        semantic_type_registry: SemanticTypeRegistry | None = None,
        **kwargs,
    ) -> None:
        super().__init__(source=source, upstreams=upstreams, **kwargs)

        self._table = table

        self._tag_columns = tuple(c for c in tag_columns if c in table.column_names)
        self._packet_columns = tuple(
            c for c in table.column_names if c not in tag_columns
        )

        semantic_type_registry = semantic_type_registry or default_registry
        tag_schema = pa.schema(
            f for f in self._table.schema if f.name in self._tag_columns
        )
        packet_schema = pa.schema(
            f for f in self._table.schema if f.name in self._packet_columns
        )
        self._tag_converter = SemanticConverter.from_arrow_schema(
            tag_schema, semantic_type_registry
        )
        self._packet_converter = SemanticConverter.from_arrow_schema(
            packet_schema, semantic_type_registry
        )

        self._cached_elements: list[tuple[dp.Tag, ArrowPacket]] | None = None
        self._set_modified_time()  # set modified time to now

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        return self._tag_columns, self._packet_columns

    def types(self) -> tuple[schemas.PythonSchema, schemas.PythonSchema]:
        """
        Returns the types of the tag and packet columns in the stream.
        This is useful for accessing the types of the columns in the stream.
        """
        # TODO: consider using MappingProxyType to avoid copying the dicts
        return (
            self._tag_converter.python_schema.copy(),
            self._packet_converter.python_schema.copy(),
        )

    def as_table(self) -> pa.Table:
        """
        Returns the underlying table representation of the stream.
        This is useful for converting the stream to a table format.
        """
        return self._table

    def clear_cache(self) -> None:
        """
        Resets the cached elements of the stream.
        This is useful for re-iterating over the stream.
        """
        self._cached_elements = None

    def iter_packets(self) -> Iterator[tuple[dp.Tag, ArrowPacket]]:
        """
        Iterates over the packets in the stream.
        Each packet is represented as a tuple of (Tag, Packet).
        """
        if self._cached_elements is None:
            self._cached_elements = []
            tags = self._table.select(self._tag_columns)
            packets = self._table.select(self._packet_columns)
            for tag_batch, packet_batch in zip(tags.to_batches(), packets.to_batches()):
                for i in range(len(tag_batch)):
                    self._cached_elements.append(
                        (
                            ArrowTag(tag_batch.slice(i, 1)),
                            ArrowPacket(packet_batch.slice(i, 1)),
                        )
                    )
        yield from self._cached_elements

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}(table={self._table.column_names}, "
            f"tag_columns={self._tag_columns})"
        )


class PodStream(StreamBase):
    def __init__(
        self,
        pod: dp.Pod,
        input_stream: dp.Stream,
        error_handling: Literal["raise", "ignore", "warn"] = "raise",
        **kwargs,
    ) -> None:
        super().__init__(upstreams=(input_stream,), **kwargs)
        self.pod = pod
        self.input_stream = input_stream
        self.error_handling = error_handling
        self._source = pod

        # Cache for processed packets
        # This is a dictionary mapping the index of the packet in the input stream to a tuple of (Tag, Packet)
        # This allows us to efficiently access the processed packets without re-processing them
        self._cached_output_packets: dict[int, tuple[dp.Tag, dp.Packet]] = {}
        self._computation_complete: bool = False
        self._cached_output_table: pa.Table | None = None

    @property
    def source(self) -> dp.Pod | None:
        """
        The source of the stream, which is the pod that generated the stream.
        This is typically used to track the origin of the stream in the computational graph.
        """
        return self._source

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        tag_keys, _ = self.input_stream.keys()
        packet_keys = tuple(self.pod.output_packet_types().keys())
        return tag_keys, packet_keys

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        tag_typespec, _ = self.input_stream.types()
        # TODO: check if copying can be avoided
        packet_typespec = dict(self.pod.output_packet_types())
        return tag_typespec, packet_typespec

    def clear_cache(self) -> None:
        """
        Clears the cached results of the processed stream.
        This is useful for re-processing the stream with the same processor.
        """
        self._cached_output_packets = {}
        self._computation_complete = False
        self._cached_output_table = None

    def refresh(self, force: bool = False) -> bool:
        if not self.is_current or force:
            self.invalidate()
            return True
        return False

    def invalidate(self) -> None:
        """
        Invalidate the stream, marking it as needing recomputation.
        This will clear the cached stream and set the last modified time to None.
        """
        self.clear_cache()
        self._set_modified_time(invalidate=True)

    def as_table(self) -> pa.Table:
        self.refresh()
        if self._cached_output_table is None:
            all_tags = []
            all_packets = []
            for tag, packet in self.iter_packets():
                # TODO: evaluate handling efficiency here
                all_tags.append(tag.as_dict())
                all_packets.append(packet.as_dict())
            all_tags: pa.Table = pa.Table.from_pylist(all_tags)
            all_packets: pa.Table = pa.Table.from_pylist(all_packets)
            # assert that column names do not overlap
            overlapping_columns = set(all_tags.column_names) & set(
                all_packets.column_names
            )
            if overlapping_columns:
                raise ValueError(
                    f"Column names overlap between tags and packets: {overlapping_columns}. Overlapping tag and packet columns are not supported yet."
                )
            self._cached_output_table = pa.Table.from_arrays(
                all_tags.columns + all_packets.columns,
                names=all_tags.column_names + all_packets.column_names,
            )

        return self._cached_output_table

    def iter_packets(self) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        self.refresh()
        if not self._computation_complete or self._cached_output_packets is None:
            for i, (tag, packet) in enumerate(self.input_stream.iter_packets()):
                if i not in self._cached_output_packets:
                    try:
                        processed_tag, processed_packet = self.pod.call(tag, packet)
                    except Exception as e:
                        logger.error(f"Error processing packet {packet}: {e}")
                        if self.error_handling == "raise":
                            raise e
                        elif self.error_handling == "warn":
                            warnings.warn(f"Error processing packet {packet}: {e}")
                            continue
                        elif self.error_handling == "ignore":
                            continue
                        else:
                            raise ValueError(
                                f"Unknown error handling mode: {self.error_handling} encountered while handling error:"
                            ) from e
                    if processed_packet is None:
                        # call returning None means the packet should be skipped
                        logger.debug(
                            f"Packet {packet} with tag {tag} was processed but returned None, skipping."
                        )
                        continue
                    self._cached_output_packets[i] = (processed_tag, processed_packet)
                    yield processed_tag, processed_packet
            self._computation_complete = True
            self._set_modified_time()

        else:
            for i in range(len(self._cached_output_packets)):
                yield self._cached_output_packets[i]
