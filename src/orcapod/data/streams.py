import logging
from pathlib import Path
from abc import ABC, abstractmethod
from collections.abc import Collection, Iterator
from datetime import datetime, timezone
from itertools import repeat
from typing import TYPE_CHECKING, Any

from orcapod.data.base import LabeledContentIdentifiableBase
from orcapod.data.context import DataContext
from orcapod.data.datagrams import (
    ArrowPacket,
    ArrowTag,
    DictTag,
)
from orcapod.data.system_constants import orcapod_constants as constants
from orcapod.protocols import data_protocols as dp
from orcapod.types import TypeSpec, schemas
from orcapod.types.semantic_converter import SemanticConverter
from orcapod.utils import arrow_utils
from orcapod.utils.lazy_module import LazyModule

if TYPE_CHECKING:
    import pyarrow as pa
else:
    pa = LazyModule("pyarrow")

# TODO: consider using this instead of making copy of dicts
# from types import MappingProxyType

logger = logging.getLogger(__name__)


class StreamBase(ABC, LabeledContentIdentifiableBase):
    """
    A stream is a collection of tagged-packets that are generated by an operation.
    The stream is iterable and can be used to access the packets in the stream.

    A stream has property `invocation` that is an instance of Invocation that generated the stream.
    This may be None if the stream is not generated by a kernel (i.e. directly instantiated by a user).
    """

    def __init__(
        self,
        source: dp.Kernel | None = None,
        upstreams: tuple[dp.Stream, ...] = (),
        data_context: str | DataContext | None = None,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self._source = source
        self._upstreams = upstreams
        self._last_modified: datetime | None = None
        self._set_modified_time()
        # note that this is not necessary for Stream protocol, but is provided
        # for convenience to resolve semantic types and other context-specific information
        if data_context is None and source is not None:
            # if source is provided, use its data context
            data_context = source.data_context_key
        self._data_context = DataContext.resolve_data_context(data_context)

    @property
    def data_context(self) -> DataContext:
        """
        Returns the data context for the stream.
        This is used to resolve semantic types and other context-specific information.
        """
        return self._data_context

    @property
    def source(self) -> dp.Kernel | None:
        """
        The source of the stream, which is the kernel that generated the stream.
        This is typically used to track the origin of the stream in the computational graph.
        """
        return self._source

    @property
    def upstreams(self) -> tuple[dp.Stream, ...]:
        """
        The upstream streams that are used to generate this stream.
        This is typically used to track the origin of the stream in the computational graph.
        """
        return self._upstreams

    def computed_label(self) -> str | None:
        if self.source is not None:
            # use the invocation operation label
            return self.source.label
        return None

    @abstractmethod
    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]: ...

    @abstractmethod
    def types(self) -> tuple[TypeSpec, TypeSpec]: ...

    @property
    def last_modified(self) -> datetime | None:
        """
        Returns when the stream's content was last modified.
        This is used to track the time when the stream was last accessed.
        Returns None if the stream has not been accessed yet.
        """
        return self._last_modified

    @property
    def is_current(self) -> bool:
        """
        Returns whether the stream is current.
        A stream is current if the content is up-to-date with respect to its source.
        This can be used to determine if a stream with non-None last_modified is up-to-date.
        Note that for asynchronous streams, this status is not applicable and always returns False.
        """
        if self.last_modified is None:
            # If there is no last_modified timestamp, we cannot determine if the stream is current
            return False

        # check if the source kernel has been modified
        if self.source is not None and (
            self.source.last_modified is None
            or self.source.last_modified > self.last_modified
        ):
            return False

        # check if all upstreams are current
        for upstream in self.upstreams:
            if (
                not upstream.is_current
                or upstream.last_modified is None
                or upstream.last_modified > self.last_modified
            ):
                return False
        return True

    def _set_modified_time(
        self, timestamp: datetime | None = None, invalidate: bool = False
    ) -> None:
        if invalidate:
            self._last_modified = None
            return

        if timestamp is not None:
            self._last_modified = timestamp
        else:
            self._last_modified = datetime.now(timezone.utc)

    def __iter__(
        self,
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        return self.iter_packets()

    @abstractmethod
    def iter_packets(
        self,
    ) -> Iterator[tuple[dp.Tag, dp.Packet]]: ...

    @abstractmethod
    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_content_hash: bool | str = False,
    ) -> pa.Table: ...

    def flow(self) -> Collection[tuple[dp.Tag, dp.Packet]]:
        """
        Flow everything through the stream, returning the entire collection of
        (Tag, Packet) as a collection. This will tigger any upstream computation of the stream.
        """
        return [e for e in self]

    # --------------------- Recursive methods ---------------------------
    # These methods form a step in the multi-class recursive invocation that follows the pattern of
    # Stream -> Invocation -> Kernel -> Stream ... -> Invocation -> Kernel
    # Most of the method logic would be found in Kernel's implementation of the method with
    # Stream and Invocation simply serving as recursive steps

    def identity_structure(self) -> Any:
        """
        Identity structure of a stream is deferred to the identity structure
        of the associated invocation, if present.
        A bare stream without invocation has no well-defined identity structure.
        Specialized stream subclasses should override this method to provide more meaningful identity structure
        """
        if self.source is not None:
            # if the stream is generated by an operation, use the identity structure from the invocation
            return self.source.identity_structure(self.upstreams)
        return super().identity_structure()


class ImmutableTableStream(StreamBase):
    """
    An immutable stream based on a PyArrow Table.
    This stream is designed to be used with data that is already in a tabular format,
    such as data loaded from a file or database. The columns to be treated as tags are
    specified at initialization, and the rest of the columns are treated as packets.
    The stream is immutable, meaning that once it is created, it cannot be modified.
    This is useful for ensuring that the data in the stream remains consistent and unchanging.

    The types of the tag and packet columns are inferred from the PyArrow Table schema.
    """

    def __init__(
        self,
        table: pa.Table,
        tag_columns: Collection[str] = (),
        source_info: dict[str, str | None] | None = None,
        source: dp.Kernel | None = None,
        upstreams: tuple[dp.Stream, ...] = (),
        **kwargs,
    ) -> None:
        super().__init__(source=source, upstreams=upstreams, **kwargs)

        data_table, data_context_table = arrow_utils.split_by_column_groups(
            table, [constants.CONTEXT_KEY]
        )
        if data_table is None:
            # TODO: provide better error message
            raise ValueError(
                "Table must contain at least one column to be used as a stream."
            )

        if data_context_table is None:
            data_context_table = pa.table(
                {constants.CONTEXT_KEY: pa.nulls(len(data_table), pa.large_string())}
            )

        prefix_info = {constants.SOURCE_PREFIX: source_info}

        # determine tag columns first and then exclude any source info
        self._tag_columns = tuple(c for c in tag_columns if c in table.column_names)
        if delta := set(tag_columns) - set(self._tag_columns):
            raise ValueError(
                f"Specified tag columns {delta} are not present in the table."
            )
        table, prefix_tables = arrow_utils.prepare_prefixed_columns(
            table, prefix_info, exclude_columns=self._tag_columns
        )
        # now table should only contain tag columns and packet columns
        self._packet_columns = tuple(
            c for c in table.column_names if c not in tag_columns
        )
        self._table = table
        self._source_info_table = prefix_tables[constants.SOURCE_PREFIX]
        self._data_context_table = data_context_table

        if len(self._packet_columns) == 0:
            raise ValueError(
                "No packet columns found in the table. At least one packet column is required."
            )

        tag_schema = pa.schema(
            f for f in self._table.schema if f.name in self._tag_columns
        )
        packet_schema = pa.schema(
            f for f in self._table.schema if f.name in self._packet_columns
        )

        self._tag_schema = tag_schema
        self._packet_schema = packet_schema
        self._tag_converter = SemanticConverter.from_semantic_schema(
            schemas.SemanticSchema.from_arrow_schema(
                tag_schema, self._data_context.semantic_type_registry
            )
        )
        self._packet_converter = SemanticConverter.from_semantic_schema(
            schemas.SemanticSchema.from_arrow_schema(
                packet_schema, self._data_context.semantic_type_registry
            )
        )

        self._cached_elements: list[tuple[dp.Tag, ArrowPacket]] | None = None
        self._set_modified_time()  # set modified time to now

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        return self._tag_columns, self._packet_columns

    def types(self) -> tuple[schemas.PythonSchema, schemas.PythonSchema]:
        """
        Returns the types of the tag and packet columns in the stream.
        This is useful for accessing the types of the columns in the stream.
        """
        # TODO: consider using MappingProxyType to avoid copying the dicts
        return (
            schemas.PythonSchema.from_arrow_schema(
                self._tag_schema, converters=self._tag_converter.as_dict()
            ),
            schemas.PythonSchema.from_arrow_schema(
                self._packet_schema, converters=self._packet_converter.as_dict()
            ),
        )

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_content_hash: bool | str = False,
    ) -> pa.Table:
        """
        Returns the underlying table representation of the stream.
        This is useful for converting the stream to a table format.
        """
        output_table = self._table
        if include_content_hash:
            hash_column_name = (
                "_content_hash"
                if include_content_hash is True
                else include_content_hash
            )
            content_hashes = [
                packet.content_hash() for _, packet in self.iter_packets()
            ]
            output_table = output_table.append_column(
                hash_column_name, pa.array(content_hashes, type=pa.large_string())
            )
        table_stack = (output_table,)
        if include_data_context:
            table_stack += (self._data_context_table,)
        if include_source:
            table_stack += (self._source_info_table,)
        return arrow_utils.hstack_tables(*table_stack)

    def clear_cache(self) -> None:
        """
        Resets the cached elements of the stream.
        This is useful for re-iterating over the stream.
        """
        self._cached_elements = None

    def iter_packets(self) -> Iterator[tuple[dp.Tag, ArrowPacket]]:
        """
        Iterates over the packets in the stream.
        Each packet is represented as a tuple of (Tag, Packet).
        """
        # TODO: make it work with table batch stream
        if self._cached_elements is None:
            self._cached_elements = []
            tag_present = len(self._tag_columns) > 0
            if tag_present:
                tags = self._table.select(self._tag_columns)
                tag_batches = tags.to_batches()
            else:
                tag_batches = repeat(DictTag({}))

            # TODO: come back and clean up this logic

            packets = self._table.select(self._packet_columns)
            for tag_batch, packet_batch in zip(tag_batches, packets.to_batches()):
                for i in range(len(packet_batch)):
                    if tag_present:
                        tag = ArrowTag(
                            tag_batch.slice(i, 1),  # type: ignore
                            semantic_converter=self._tag_converter,
                            data_context=self._data_context,
                        )

                    else:
                        tag = tag_batch
                    self._cached_elements.append(
                        (
                            tag,
                            ArrowPacket(
                                packet_batch.slice(i, 1),
                                semantic_converter=self._packet_converter,
                                data_context=self._data_context,
                            ),
                        )
                    )
        yield from self._cached_elements

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}(table={self._table.column_names}, "
            f"tag_columns={self._tag_columns})"
        )


class KernelStream(StreamBase):
    """
    Recomputable stream that wraps a stream produced by a kernel to provide
    an abstraction over the stream, taking the stream's source and upstreams as the basis of
    recomputing the stream.

    This stream is used to represent the output of a kernel invocation.
    """

    def __init__(
        self,
        output_stream: dp.Stream | None = None,
        source: dp.Kernel | None = None,
        upstreams: tuple[
            dp.Stream, ...
        ] = (),  # if provided, this will override the upstreams of the output_stream
        **kwargs,
    ) -> None:
        if (output_stream is None or output_stream.source is None) and source is None:
            raise ValueError(
                "Either output_stream must have a kernel assigned to it or source must be provided in order to be recomputable."
            )
        if source is None:
            if output_stream is None or output_stream.source is None:
                raise ValueError(
                    "Either output_stream must have a kernel assigned to it or source must be provided in order to be recomputable."
                )
            source = output_stream.source
            upstreams = upstreams or output_stream.upstreams

        super().__init__(source=source, upstreams=upstreams, **kwargs)
        self.kernel = source
        self._cached_stream = output_stream

    def clear_cache(self) -> None:
        """
        Clears the cached stream.
        This is useful for re-processing the stream with the same kernel.
        """
        self._cached_stream = None
        self._set_modified_time(invalidate=True)

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        tag_types, packet_types = self.kernel.output_types(*self.upstreams)
        return tuple(tag_types.keys()), tuple(packet_types.keys())

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        """
        Returns the types of the tag and packet columns in the stream.
        This is useful for accessing the types of the columns in the stream.
        """
        return self.kernel.output_types(*self.upstreams)

    @property
    def is_current(self) -> bool:
        if self._cached_stream is None or not super().is_current:
            status = self.refresh()
            if not status:  # if it failed to update for whatever reason
                return False
        return True

    def refresh(self, force: bool = False) -> bool:
        updated = False
        if force or (self._cached_stream is not None and not super().is_current):
            self.clear_cache()

        if self._cached_stream is None:
            assert self.source is not None, (
                "Stream source must be set to recompute the stream."
            )
            self._cached_stream = self.source.forward(*self.upstreams)
            self._set_modified_time()
            updated = True

        if self._cached_stream is None:
            # TODO: use beter error type
            raise ValueError(
                "Stream could not be updated. Ensure that the source is valid and upstreams are correct."
            )

        return updated

    def invalidate(self) -> None:
        """
        Invalidate the stream, marking it as needing recomputation.
        This will clear the cached stream and set the last modified time to None.
        """
        self.clear_cache()
        self._set_modified_time(invalidate=True)

    @property
    def last_modified(self) -> datetime | None:
        if self._cached_stream is None:
            return None
        return self._cached_stream.last_modified

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_content_hash: bool | str = False,
    ) -> pa.Table:
        self.refresh()
        assert self._cached_stream is not None, (
            "Stream has not been updated or is empty."
        )
        return self._cached_stream.as_table(
            include_data_context=include_data_context,
            include_source=include_source,
            include_content_hash=include_content_hash,
        )

    def iter_packets(self) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        self.refresh()
        assert self._cached_stream is not None, (
            "Stream has not been updated or is empty."
        )
        return self._cached_stream.iter_packets()

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(kernel={self.source}, upstreams={self.upstreams})"


class LazyPodResultStream(StreamBase):
    """
    A fixed stream that lazily processes packets from a prepared input stream.
    This is what Pod.process() returns - it's static/fixed but efficient.
    """

    def __init__(self, pod: dp.Pod, prepared_stream: dp.Stream, **kwargs):
        super().__init__(source=pod, upstreams=(prepared_stream,), **kwargs)
        self.pod = pod
        self.prepared_stream = prepared_stream
        self._set_modified_time()  # set modified time to when we obtain the iterator
        # capture the immutable iterator from the prepared stream
        self._prepared_stream_iterator = prepared_stream.iter_packets()

        # Packet-level caching (from your PodStream)
        self._cached_output_packets: dict[int, tuple[dp.Tag, dp.Packet | None]] = {}
        self._cached_output_table: pa.Table | None = None

    def iter_packets(self) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        if self._prepared_stream_iterator is not None:
            for i, (tag, packet) in enumerate(self._prepared_stream_iterator):
                if i in self._cached_output_packets:
                    # Use cached result
                    tag, packet = self._cached_output_packets[i]
                    if packet is not None:
                        yield tag, packet
                else:
                    # Process packet
                    processed = self.pod.call(tag, packet)
                    if processed is not None:
                        # Update shared cache for future iterators (optimization)
                        self._cached_output_packets[i] = processed
                        tag, packet = processed
                        if packet is not None:
                            yield tag, packet

            # Mark completion by releasing the iterator
            self._prepared_stream_iterator = None
        else:
            # Yield from snapshot of complete cache
            for i in range(len(self._cached_output_packets)):
                tag, packet = self._cached_output_packets[i]
                if packet is not None:
                    yield tag, packet

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """

        tag_keys, _ = self.prepared_stream.keys()
        packet_keys = tuple(self.pod.output_packet_types().keys())
        return tag_keys, packet_keys

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        tag_typespec, _ = self.prepared_stream.types()
        # TODO: check if copying can be avoided
        packet_typespec = dict(self.pod.output_packet_types())
        return tag_typespec, packet_typespec

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_content_hash: bool | str = False,
    ) -> pa.Table:
        if self._cached_output_table is None:
            all_tags = []
            all_packets = []
            tag_schema, packet_schema = None, None
            for tag, packet in self.iter_packets():
                if tag_schema is None:
                    tag_schema = tag.arrow_schema()
                if packet_schema is None:
                    packet_schema = packet.arrow_schema(
                        include_context=True,
                        include_source=True,
                    )
                all_tags.append(tag.as_dict())
                # FIXME: using in the pinch conversion to str from path
                # replace with an appropriate semantic converter-based approach!
                dict_patcket = packet.as_dict(include_context=True, include_source=True)
                for k, v in dict_patcket.items():
                    if isinstance(v, Path):
                        dict_patcket[k] = str(v)
                all_packets.append(dict_patcket)

            # FIXME: this skips the semantic version conversion and thus is not
            # fully correct!
            all_tags_as_tables: pa.Table = pa.Table.from_pylist(
                all_tags, schema=tag_schema
            )
            all_packets_as_tables: pa.Table = pa.Table.from_pylist(
                all_packets, schema=packet_schema
            )

            self._cached_output_table = arrow_utils.hstack_tables(
                all_tags_as_tables, all_packets_as_tables
            )
        assert self._cached_output_table is not None, (
            "_cached_output_table should not be None here."
        )

        drop_columns = []
        if not include_source:
            drop_columns.extend(f"{constants.SOURCE_PREFIX}{c}" for c in self.keys()[1])
        if not include_data_context:
            drop_columns.append(constants.CONTEXT_KEY)

        output_table = self._cached_output_table.drop(drop_columns)

        # lazily prepare content hash column if requested
        if include_content_hash:
            if self._cached_content_hash_column is None:
                content_hashes = []
                for tag, packet in self.iter_packets():
                    content_hashes.append(packet.content_hash())
                self._cached_content_hash_column = pa.array(
                    content_hashes, type=pa.large_string()
                )
            assert self._cached_content_hash_column is not None, (
                "_cached_content_hash_column should not be None here."
            )
            hash_column_name = (
                "_content_hash"
                if include_content_hash is True
                else include_content_hash
            )
            output_table = output_table.append_column(
                hash_column_name, self._cached_content_hash_column
            )
        return output_table


class WrappedStream(StreamBase):
    def __init__(
        self,
        stream: dp.Stream,
        source: dp.Kernel,
        input_streams: tuple[dp.Stream, ...],
        label: str | None = None,
        **kwargs,
    ) -> None:
        super().__init__(source=source, upstreams=input_streams, label=label, **kwargs)
        self._stream = stream

    def keys(self) -> tuple[tuple[str, ...], tuple[str, ...]]:
        """
        Returns the keys of the tag and packet columns in the stream.
        This is useful for accessing the columns in the stream.
        """
        return self._stream.keys()

    def types(self) -> tuple[TypeSpec, TypeSpec]:
        """
        Returns the types of the tag and packet columns in the stream.
        This is useful for accessing the types of the columns in the stream.
        """
        return self._stream.types()

    def as_table(
        self,
        include_data_context: bool = False,
        include_source: bool = False,
        include_content_hash: bool | str = False,
    ) -> pa.Table:
        """
        Returns the underlying table representation of the stream.
        This is useful for converting the stream to a table format.
        """
        return self._stream.as_table(
            include_data_context=include_data_context,
            include_source=include_source,
            include_content_hash=include_content_hash,
        )

    def iter_packets(self) -> Iterator[tuple[dp.Tag, dp.Packet]]:
        """
        Iterates over the packets in the stream.
        Each packet is represented as a tuple of (Tag, Packet).
        """
        return self._stream.iter_packets()

    def identity_structure(self) -> Any:
        return self._stream.identity_structure()
